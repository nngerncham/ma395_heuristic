\section{Tabu Search for MST}

\subsection{Implementation}

The full implementation for the algorithm by itself can be found \href{https://github.com/nngerncham/ma395_heuristic/blob/main/homework/hw3/codebase/tabu_search/algorithm.py}{here}. In short, the algorithm takes in the cost function, initial solution \(s_0\), the max number of iterations (defaults to 200), and a class that implements the neighbor generation and tabu checking. This was done in so that I can reuse the code for this portion in later parts. This came back to bite me in the *** later on :(

The algorithm itself is implemented according to the algorithm in the slides (slide 5, page 8). Specifically, it only checks the best neighbor in each iteration and if every neighbor is tabu and doesn't meet the aspiration criterion, it just continues on to the next iteration without re-generating the neighborhood.

The neighbor and tabu list class (\texttt{NeighborsTabuListInterface}) simply makes sure that for whatever configuration we want for the tabu list, we can make it without having to rewrite the algorithm. Namely, it takes in the tenure length \(|T|\), neighbor sample size \(|V^*|\), and the cost function and supports adding the attribute to \(T\), generating the neighborhood, evaluating a solution (is the cost function by default but can be changed for probabilitic TS), and checking if an attribute is tabu or not.

Then, the problem-specific setup (cost function, neighbor tabu list class, etc.) can be found in \href{https://github.com/nngerncham/ma395_heuristic/blob/main/homework/hw3/codebase/Notebooks/Problem\%201.ipynb}{this Jupyter Notebook}.

\subsection{Experiments}

Each experiment in this section is run 200 times. At the start, the default parameters are as follows:
\begin{itemize}
    \item \(|T| = 3\)
    \item \(|V^*| = 4\)
    \item Attribute stored is the newly added edge
\end{itemize}

\subsubsection{Only changing \(|T|\)}

The progress plots can be seen in Figure \ref{fig:p1-change-tenure}. From the plots, we can make the following observations:
\begin{itemize}
    \item Shorter tenure length results in faster convergence to exact optimal (within 60 iterations)
    \item Longer tenure length results in increasing current cost progress
    \item Longer tenure length also results in bigger standard deviation for the best cost and current cost progress
\end{itemize}
From these observations, we will choose \(|T| = 2\) as our best tenure length for this problem since it is the only that guarantees convergence.

\begin{figure}[ht]
    \centering
    \hfill
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../images/p1/added24s.png}
        \caption{\(|T| = 2\)}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../images/p1/added34s.png}
        \caption{\(|T| = 3\) (handout)}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../images/p1/added64s.png}
        \caption{\(|T| = 6\)}
    \end{subfigure}
    \hfill

    \caption{Effects changing \(|T|\)}
    \label{fig:p1-change-tenure}
\end{figure}

\subsubsection{Only changing \(|V^*|\)}

The progress plots are in Figure \ref{fig:p1-change-nsize}. From the plots, we can see that larger neighborhood size results in both faster convergence and lower average current cost progress. It also results in smaller standard deviation in the current cost progress as well. However, it does come with higher running time as show in Table \ref{table:sample-time}.

So, while higher neighborhood size leads to higher accuracy, it also takes longer to run. Thus, we will pick \(|V^*|=4\) to be our best neighborhood size for this problem since it offers a good balance between running time (around 1 second for 200 trials of 100 iterations) while still exploring more than \(|V^*| = 2\) in each iteration.

\begin{table}[ht]
    \centering
    \begin{tabular}{c c}
        \toprule
        \(|V^*|\) & Running time \\
        \midrule
        2 & 890ms \\
        4 & 1s 138ms \\
        8 & 1s 715ms \\
        \bottomrule
    \end{tabular}
    \caption{Running time based on \(|V^*|\)}
    \label{table:sample-time}
\end{table}

\begin{figure}[ht]
    \centering
    \hfill
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../images/p1/added32s.png}
        \caption{\(|V^*| = 2\)}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../images/p1/added34s.png}
        \caption{\(|V^*| = 4\) (handout)}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../images/p1/added38s.png}
        \caption{\(|V^*| = 8\)}
    \end{subfigure}
    \hfill

    \caption{Effects changing \(|V^*|\)}
    \label{fig:p1-change-nsize}
\end{figure}

\subsubsection{Only changing the attribute stored}

The progress plots are in Figure \ref{fig:p1-change-attr}. From the plots, we can make the following observations:
\begin{itemize}
    \item Storing single edge leads to lower standard deviation for the best cost progress
    \item Storing the deleted edge starts with higher standard deviation but becomes lower than storing the added edge after around 50 iterations
    \item In contrast, storing a single edge leads to higher and more chaotic standard deviation of the current cost progress
    \item The average current and best cost progress lines when storing both edges pretty much have constant difference after some point
\end{itemize}

So, we can see that storing a single edge is far better than storing both. This could be because storing both edges makes the tabu criteria more stringent by storing and checking both edges, leading to less chance for a move to be tabu. This means that TS behaves more similar to greedy local search which can easily get stuck in a local optimum. This might not be so useful since the solutions for MST can have the same values which could prevent exploration from happening.

Thus, we will pick the added edge to be the attribute stored as it offers a more gradual change of standard deviation over deleted edge where there is a sharp drop at around iteration 50.

\begin{figure}[ht]
    \centering
    \hfill
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../images/p1/added34s.png}
        \caption{Storing added edge}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../images/p1/deleted34s.png}
        \caption{Storing deleted edge}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../images/p1/both34s.png}
        \caption{Storing both edges}
    \end{subfigure}
    \hfill

    \caption{Effects changing the attribute stored}
    \label{fig:p1-change-attr}
\end{figure}

\subsubsection{\textit{Best} parameters}

Putting the previous three sections together, below is the result of what I consider to be the best parameters for using Tabu Search on this instance of MST. Namely, we will use \(|T| = 2\), \(|V^*| = 4\), and storing the added edge. Below is the progress plot.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{../images/p1/best.png}
    \caption{\(|T| = 2\), \(|V^*| = 4\), storing the added edge}
    \label{fig:p1-best}
\end{figure}

With this parameter, TS converges to the exact optimal every time, and all of them converges within 80 iterations.
