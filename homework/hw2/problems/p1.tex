\section*{Problem 1}

\subsection*{Part 1: Implementing the Simple Simulated Annealing Algorithm for TSP problems}

The full implimentation can be found \href{https://github.com/nngerncham/ma395_heuristic/tree/main/homework/hw2/code/simulated_annealing}{here}. The simulated annealing algorithm itself is implemented in such a way that the parameters \(T_0, \alpha\), and the max number of iterations can be tweaked but the defaults are \(1000, 0.95,\) and 5000, respectively.

The algorithm itself takes as arguments the objective function, the neighbor function, and the initial solution. The constraint is that the objective function must be \(f: \mathbb{R}^d \to \mathbb{R}\), and the neighbor function must be \(N: \mathbb{R}^d \to \mathbb{R}^d\). That is, both takes in a vector/list/array of numbers and returns a number for the objective function and a vector/list/array for the neighbor function. For TSP, the cities are encoded as integers and stored as an array. The implementation for the neighbor function can be found \href{https://github.com/nngerncham/ma395_heuristic/blob/main/homework/hw2/code/simulated_annealing/tsp_neighbors.py}{here}. Following is a quick round-up of parameters tested.
\begin{itemize}
    \item \(T_0\)'s tested are 10, 50 100, 1000, 5000, 10000
    \item \(\alpha\) or cooling rate is fixed at 0.95
    \item Maximum number of iterations are 1500, 2500, and 5000 for GR17, FRI26, and ATT48, respectively
    \item The number of repeats the experiments are run is 30 and the data used for HC is from the previous homework
\end{itemize}

\subsection*{Part 2: Solving GR17, FRI26, and ATT48}

\subsubsection*{Simulated annealing on its own}

\begin{figure}
    \centering
    \includegraphics[height=0.5\textheight]{images/gr17_sa.png}
    \caption{Simulated annealing results on GR17}
    \label{fig:gr17-sa}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[height=0.4\textheight]{images/gr17-no-error.png}
    \caption{Simulated annealing results on GR17 with no error bar}
    \label{fig:gr17-ne}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[height=0.5\textheight]{images/fri26_sa.png}
    \caption{Simulated annealing results on FRI26}
    \label{fig:fri26-sa}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[height=0.4\textheight]{images/fri26-no-error.png}
    \caption{Simulated annealing results on FRI26 with no error bar}
    \label{fig:fri26-ne}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[height=0.5\textheight]{images/att48_sa.png}
    \caption{Simulated annealing results on ATT48}
    \label{fig:att48-sa}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[height=0.4\textheight]{images/att48-no-error.png}
    \caption{Simulated annealing results on ATT48 with no error bar}
    \label{fig:att48-ne}
\end{figure}

From the Figures \ref{fig:gr17-sa}, \ref{fig:fri26-sa}, and \ref{fig:att48-sa} (and more clearly in \ref{fig:gr17-ne}, \ref{fig:fri26-ne}, and \ref{fig:att48-ne}), we can see that, at least for TSP problems, \(T_0\) and acceptance rate does not have that big of an effect on the final best cost. While there is some slight difference in the final best cost on GR17 and FRI26, all best costs essentially approach the same value on ATT48.

What \(T_0\) does affect, however, is how fast the best cost decreases over time. Since, with higher \(T_0\), the algorithm accepts worse solutions for longer which could cause their neighbors to be worse as well. That is, higher \(T_0\) leads to higher early best costs but not better final best costs. Hence, it is safe to say that setting \(T_0\) too high is not always ideal as it would require more time to get a good-enough solution compared to other values.

\subsubsection*{Simulated annealing compared to hill climbing}

\begin{figure}
    \centering
    \includegraphics[height=0.27\textheight]{images/gr17-with-hc.png}
    \caption{Simulated annealing and hill climbing on GR17}
    \label{fig:gr17}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[height=0.27\textheight]{images/fri26-with-hc.png}
    \caption{Simulated annealing and hill climbing on FRI26}
    \label{fig:fri26}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[height=0.27\textheight]{images/att48-with-hc.png}
    \caption{Simulated annealing and hill climbing on ATT48}
    \label{fig:att48}
\end{figure}

The \(T_0\)'s used are 100 for all data sets it gives the best solutions for all data sets. This suggests that TSP might be a problem where exploration does not help much and it is better to keep exploiting the neighbors of same solution over and over again until any improvement is found.

% Across all data sets, we observe that 2-opt is the superior neighbor function regardless of algorithm. This is not so clear in GR17 but the difference of best costs between using 2-opt and 2-swap as the neighbor function becomes very significant on FRI26 and ATT48.

% On the other hand, given the same neighbor function, the difference of best cost between algorithms is slightly noticable with GR17. However, the difference shrinks as the data set becomes larger to the point where it is barely noticable in ATT48. However, it is still worth noting that simulated annealing gives a better final best cost on average with the same number of iterations.

% Another interesting trend seen in GR17 and FRI26 (and vaguely in ATT48) is that the best cost of hill climbing (simply denoted as Randomized in plot) variants begins lower than simulated annealing variants. However, the best cost obtained from SA is able to reach lower values within the first \(\frac{1}{5}^\text{th}\) iterations of the maximum number of iterations. This suggests that SA performs better than HC given sufficient number of iterations where sufficient number of iterations is not very high.

\subsubsection*{Conclusion}

From the experiments, we can conclude that 2-opt is the better neighbor function and, among the two algorithms, simulated annealing seems to be the better one as it is able to give the best final best cost within the same number of iterations. This comes with the caveat that there are multiple parameters to be tweaked to fit the actual problem itself in order to produce the best results since \(T_0\) also depends on the size of the problem itself as well.

At least in the case of solving TSP, the implementation overhead and optimizing the parameters may not be worth it in some real-world applications where the user is able to tolerate higher errors but needs the solution very quickly. However, the fact that SA has tweakable parameters makes it a very powerful tool when there is more time to investigate good parameters in order to produce the best results.

\subsection*{Part 3: Selecting \(T_0\)}

\begin{table}
    \centering
    \begin{tabular}{|c|c|cc|}
        \hline
        Algorithm & \(T_0\) & Average best cost & Minimum best cost \\
        \hline
        Simulated annealing 2-opt & 10 & 35675.12 & 34337 \\
                                  & 50 & 35633.02 & 34212 \\
                                  & 100 & 35599.67 & 33809 \\
                                  & 500 & 35633.02 & 34212 \\
                                  & 1000 & 35683.67 & 33888 \\
                                  & 5000 & 35735.46 & 34047 \\
                                  & 10000 & 35639.81 & 33759 \\
                                  & 25000 & 35796.26 & 34086 \\
                                  & 50000 & 35735.73 & 33653 \\
        \hline
        Simulated annealing 2-swap & 10 & 49757.11 & 39018 \\
                                   & 50 & 49798.82 & 41174 \\
                                   & 100 & 49757.15 & 39984 \\
                                   & 500 & 49920.80 & 39715 \\
                                   & 1000 & 49333.90 & 38801 \\
                                   & 5000 & 49307.98 & 39183 \\
                                   & 10000 & 49872.49 & 42594 \\
                                   & 25000 & 50324.74 & 42313 \\
                                   & 50000 & 49595.99 & 38887 \\
        \hline
    \end{tabular}
    \caption{Average and minimum best cost on ATT48}
    \label{table:avg-min-att48}
\end{table}

As briefly mentioned before, simulated annealing was tested on \(T_0 = 10, 50, 100, 1000, 5000, 10000\) on GR17 and FRI26. \(T_0 = \) 25000 and 50000 were also tested for ATT48. In Table \ref{table:avg-min-att48}, the average best cost is the average value of the best cost in the last iteration of the algorithm. Minimum best cost is the lowest best cost across every trial for a given \(T_0\).

From the experiments results in Figure \ref{fig:gr17}, \ref{fig:fri26}, and \ref{fig:att48}, we can see that higher acceptance rate at the start does not lead to better final solutions as they seem to approach similar values. This is more evident in ATT48 as it has a data set big enough that reaching the true optimal is very difficult. However, using simulated annealing with 2-opt on GR17 and FRI26 gives the optimal values no matter what \(T_0\) is.

In Table \ref{table:avg-min-att48}, we can see that the average best cost for both algorithms are roughly the same. Using SA with 2-opt, we obtain an average best cost of \(\approx 35600\) to \(35800\). While \(T_0=10000\) gives the lowest minimum best cost of any trial (cost \(= 33759\)), it is not very different from the minimum best costs of other \(T_0\)'s (costs \(\approx 34000\)) and this result might have just happened by chance since we are picking the lowest across all trials.

Using SA with 2-swap, we obtain the average best cost of around \(\approx 49000\) to \(50000\). The lowest minimum best cost is obtained when \(T_0 = 1000\) with cost \(38801\). Still, this might have just happened by chance.

In conclusion, at least for TSP problems, \(T_0\) does not have that much impact on the accuracy of the solutions produced by simulated annealing, but simulated annealing is still better than hill climbing since it allows for more exploration.
