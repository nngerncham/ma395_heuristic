\section{(Fake) MOO-ing with Bayesian Optimization}

\subsection{Setup}

Since the only version of Bayesian Optimization that we know of only works on single-valued objective functions, the weighted sum of objective functions is used. Namely, for the parameter configuration \(bp\), the objective function \(f\) is defined by
\[
    f(bp) = w_1 \cdot \bar{f_c}(bp) + w_2 \cdot \bar{f_s}(bp) + w_3 \cdot (1 - f_r(bp))
\]
where the bar signifies that that objective function is normalized and \(w_1, w_2, w_3\) are different weights. The weights used in experiments were:
\begin{itemize}
    \item \([1, 1, 1]\), equal priority
    \item \([2, 1, 1]\), build priority
    \item \([1, 2, 2]\), search priority
    \item \([1, 2, 3]\), recall over search over build time
\end{itemize}

\subsection{Results}

The results shown are from running BO in 20 trials with 20 iterations per trial for each weight. I was able to track the actual values of each objective value before it was put into the weighted sum to be used in BO. The points that show up on every iteration are collected and are thrown into NSGA-II's fast non-dominating sort and the points used in the plots are only from the first frontiers.

\subsubsection{Best of BO}

In Figure \ref{fig:bo-first-frontiers}, we can see that even with different weights, the fronts that we were able to obtain from BO seem about the same. Even if we put all the points into the same plot, the distribution of points still look the same across all weights as shown in Figure \ref{fig:bo-all-frontiers}.

\begin{figure}[ht]
    \centering
    \hfill
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{../images/report/bo-frontier111.png}
        \caption{\(W = [1, 1, 1]\)}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{../images/report/bo-frontier211.png}
        \caption{\(W = [2, 1, 1]\)}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{../images/report/bo-frontier122.png}
        \caption{\(W = [1, 2, 2]\)}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{../images/report/bo-frontier123.png}
        \caption{\(W = [1, 2, 3]\)}
    \end{subfigure}
    \hfill
    \caption{First frontiers of each weight}
    \label{fig:bo-first-frontiers}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.49\textwidth]{../images/report/bo-frontiers-tgt.png}
    \caption{First frontiers of all weights}
    \label{fig:bo-all-frontiers}
\end{figure}

Again, to better quantify the quality of these obtained fronts, we will also compute the hypervolume of the domination area of these fronts. In Table \ref{tbl:hv-bo-all}, we can see that the hypervolumes are roughly the same for weights \([2, 1, 1]\) and \([1, 2, 2]\). However, the other two weights, \([1, 1, 1]\) and \([1, 2, 3]\), are quite close but still not as good as them.

\begin{table}[ht]
    \centering
    \caption{Hypervolume of each weight}
    \label{tbl:hv-bo-all}
    \begin{tabular}{cc}
        \toprule
        Weight & Hypervolume \\
        \midrule
        \([1, 1, 1]\) & 0.9408989688628545 \\
        \([2, 1, 1]\) & 0.9572758903387226 \\
        \([1, 2, 2]\) & 0.9592294524746712 \\
        \([1, 2, 3]\) & 0.9329997480019254 \\
        \bottomrule
    \end{tabular}
\end{table}

Now, let us identify the \textit{best} points, again, by using the hypervolume as the indicator for quality. The points here are drawn from all weights since, at the end, the objective values that we actually care about is not the weighted sum. In Table \ref{tbl:bo-top10}, we still see that \(M\) is very all over the place which could be another indicator that the objective values are not very sensitive to it. Meanwhile, \(C\) is very consistent at either 102 or 1024. Similarly, \(\alpha\) is mostly 1 with the two exceptions at the top. Interestingly, similar to \(M\), the values of \(S\) is also very all over the place. At least for this instance of this problem, \(M\) and \(S\) are \textit{consistently inconsistent}.

Notice that the top three results in Table \ref{tbl:bo-top10} have really low build and search time but they also have significantly worse recall. Although they have the highest domination hypervolume, I would not say that they perform the best. The real best ones seem to be the rest since they have a better balance of construction time, search time, and recall.

\begin{table}[ht]
    \centering
    \caption{Top 10 parameters sorted by hypervolume obtained from BO}
    \label{tbl:bo-top10}
    \begin{tabular}{cccccccc}
        \toprule
        \(M\) & \(C\) & \(S\) & \(\alpha\) & \(f_c\) & \(f_s\) & \(f_r\) & HV \\
        \midrule
        1 & 1024 & 102 & 2.0 & 0.056906 & 0.000900 & 0.968400 & 0.912471 \\
        1 & 1024 & 1024 & 1.632076 & 0.058899 & 0.000955 & 0.968400 & 0.910492 \\
        1 & 1024 & 502 & 1.0 & 0.059805 & 0.000902 & 0.966300 & 0.907691 \\
        714 & 102 & 1024 & 1.0 & 0.124925 & 0.003450 & 1.0 & 0.872056 \\
        833 & 102 & 343 & 1.0 & 0.127623 & 0.001628 & 1.0 & 0.870957 \\
        114 & 102 & 250 & 1.0 & 0.128544 & 0.001415 & 0.999900 & 0.870135 \\
        947 & 102 & 557 & 1.0 & 0.128413 & 0.002333 & 1.0 & 0.869553 \\
        697 & 102 & 488 & 1.0 & 0.128873 & 0.002133 & 1.0 & 0.869269 \\
        662 & 102 & 102 & 1.0 & 0.125418 & 0.000846 & 0.994400 & 0.868949 \\
        353 & 102 & 357 & 1.0 & 0.129775 & 0.001709 & 1.0 & 0.868737 \\
        \bottomrule
    \end{tabular}
\end{table}

\subsubsection{In Comparison with NSGA-II}

For ease of comparison, let us put the objective values of the top 10 points obtained from NSGA-II and BO side-by-side. Again, the top 10 points for BO are picked from all weights and the top 10 points for NSGA-II are picked from all combinations of crossover and selection methods. The last row is the mean.

\begin{table}
    \centering
    \caption{Objective values and HV of top 10 points obtained from NSGA-II and BO}
    \label{tbl:compare-nsga-bo}
    \begin{tabular}{cccc|cccc}
        \toprule
        BO & & & & NSGA-II & \\
        \(f_c\) & \(f_s\) & \(f_r\) & HV & \(f_c\) & \(f_s\) & \(f_r\) & HV \\
        \midrule
        0.056906 & 0.000900 & 0.968400 & 0.912471 & 0.018249 & 0.004453 & 0.999300 & 0.976695 \\
        0.058899 & 0.000955 & 0.968400 & 0.910492 & 0.030292 & 0.006890 & 0.995900 & 0.959078 \\
        0.059805 & 0.000902 & 0.966300 & 0.907691 & 0.001252 & 0.039502 & 0.999400 & 0.958719 \\
        0.124925 & 0.003450 & 1.0 & 0.872056 & 0.001252 & 0.039502 & 0.999400 & 0.958719 \\
        0.127623 & 0.001628 & 1.0 & 0.870957 & 0.001252 & 0.039502 & 0.999400 & 0.958719 \\
        0.128544 & 0.001415 & 0.999900 & 0.870135 & 0.001252 & 0.039502 & 0.999400 & 0.958719 \\
        0.128413 & 0.002333 & 1.0 & 0.869553 & 0.001252 & 0.039502 & 0.999400 & 0.958719 \\
        0.128873 & 0.002133 & 1.0 & 0.869269 & 0.033138 & 0.007289 & 0.995400 & 0.955399 \\
        0.125418 & 0.000846 & 0.994400 & 0.868949 & 0.033138 & 0.007289 & 0.995400 & 0.955399 \\
        0.129775 & 0.001709 & 1.0 & 0.868737 & 0.033138 & 0.007289 & 0.995400 & 0.955399 \\
        \midrule
        0.106918 & 0.001627 & 0.989740 & 0.882031 & 0.015421 & 0.023072 & 0.997840 & 0.959557 \\
        \bottomrule
    \end{tabular}
\end{table}

In Table \ref{tbl:compare-nsga-bo}, we can see that although NSGA-II's hypervolumes are higher than BO's, the parameters obtained by BO outperforms NSGA-II in terms of search time. Namely, the search time obtained by BO is significantly lower than the ones obtained by NSGA-II. However, that is the only case where BO performs better than NSGA-II. NSGA-II significantly outperforms BO in terms of build time and is still averages slightly better recall than BO despite the fact that BO obtained a few perfect recalls. This could be due to the dip in recall of the top three results.

The fact that NSGA-II generally has higher build times (\(f_c\)) and BO generally has higher search time (\(f_s\)) while both have about the same recalls can also be seen in Figure \ref{fig:bo-vs-nsga2}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.49\textwidth]{../images/report/bo-vs-nsga2.png}
    \caption{Fronts of NSGA-II vs BO}
    \label{fig:bo-vs-nsga2}
\end{figure}

While inconclusive which one is absolutely better, I would still go with NSGA-II since it seems to be more purpose-built for MOO problems. Another choice would be the BO variant that truly supports MOO but I did not look into that.
