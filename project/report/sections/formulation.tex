\section{Formulation}

Since these parameters affect the construction time, search time, and search quality (recall) the graph, a natural question arises: How should we set them to make DiskANN perform best? In this section, we will describe how the problem of optimizing the parameters of DiskANN is formulated as an concrete optimization problem.

\subsection{Decision Variables}

From the last section, we can identify the following parameters from the three algorithms used in DiskANN:
\[
\begin{aligned}
    M &= \text{Maximum degree of \textit{each} vertex in \(G\)} \\
    C &= \text{Size of the candidate set during construction} \\
    S &= \text{Size of the candidate set during search} \\
    \alpha &= \text{Distance threshold for \textsc{RobustPrune}}
\end{aligned}
\]
We then apply the following constraints:
\[
\begin{aligned}
    &1 \leq M \leq 1024, & M \in \mathbb{Z} \\
    &100 \leq C \leq 1024, & C \in \mathbb{Z} \\
    &100 \leq S \leq 1024, & S \in \mathbb{Z} \\
    &1 \leq \alpha < 2, &\alpha \in \mathbb{R}
\end{aligned}
\]
The reasoning are as follows. If \(M, C\), and \(S\) are higher than 1024, then it would be impractical to be creating a graph like this since it would be look through too many points during search anyways. Obviously, a vertex must have at least 1 edge otherwise the graph could be disconnected. Then, the lower bounds for \(C\) and \(S\) are so that we are able to perform 100-NNS which seems to be the most popular value of \(k\). Finally, the bounds for \(\alpha\) are just to match the bounds allowed in the implementation.

\subsection{Objective Functions}

In ANNS problems, we are concerned with three values: construction time, search latency, and search quality. Let \(bp\) denote a configuration of the parameters. We can define the following objective functions.
\[
\begin{aligned}
    f_c(bp) &= \text{Time taken for constructing the graph, to minimize} \\
    f_s(bp) &= \text{Time (latency) taken to search the queries in total, to minimize} \\
    f_r(bp) &= \text{Quality of the search results, to maximize}
\end{aligned}
\]
We will treat these objective functions as black boxes (since we actually don't know how the build parameters is actually going to affect output).

\subsection{Full Model}

Finally, we arrive at the following full model.
\[
\begin{aligned}
    \min_{bp} &\left(f_c(bp), f_s(bp), 1 - f_r(bp)\right) \\
    \text{s.t.} \qquad
        &1 \leq M \leq 1024, & M \in \mathbb{Z} \\
        &100 \leq C \leq 1024, & C \in \mathbb{Z} \\
        &100 \leq S \leq 1024, & S \in \mathbb{Z} \\
        &1 \leq \alpha < 2, &\alpha \in \mathbb{R}
\end{aligned}
\]
Notice that the third objective function is 1 - \(f_r(bp)\). Since \(f_r\) actually denotes the quality, which we want to maximize, we use 1 - \(f_r\) instead to minimize it. Namely, the measure used for \(f_r\) is called recall which is computed by \(\frac{|\mathcal{K} \cup \mathcal{N}|}{|\mathcal{K}|}\) where \(\mathcal{K}\) is the true nearest neighbors of a query point and \(\mathcal{N}\) is the approximated nearest neighbors outputted by an ANNS algorithm. Note that recall is always in \([0, 1]\)

\subsection{Normalization}

By Aj's recommendation, we will also normalize (scale) the objective values of our problem so that all of them are in range \([0, 1]\). This makes it so that the \textit{contribution} of each objective values is roughly equal. This becomes very important when we move on to use Bayesian Optimization.

The normalization used in this project is quite simple. For the \(i\)-th objective value denoted \(f_i\), we normalize it to \(\bar{f_i}\) by
\[
    \bar{f_i} = \frac{f_i - \hat{z}^*_i}{\hat{z}_i^{nad} - \hat{z}_i^*}
\]
where \(\hat{z}_i^{nad}\) is the estimated Nadir point and \(\hat{z}_i^*\) is the estimated ideal point. We define these points as follows. Let \(X^*\) denote the pareto front and suppose that we have \(m\) objective functions.
\begin{itemize}
    \item Nadir point \(z_i^{nad}\) is formally defined by
        \[
            z^{nad} = \begin{pmatrix}
                \sup_{x^* \in X^*} f_1(x^*) \\
                \vdots \\
                \sup_{x^* \in X^*} f_m(x^*) \\
            \end{pmatrix}
        \]
    \item Ideal point \(z_i^*\) is formally defined by
        \[
            z^{*} = \begin{pmatrix}
                \inf_{x^* \in X^*} f_1(x^*) \\
                \vdots \\
                \inf_{x^* \in X^*} f_m(x^*) \\
            \end{pmatrix}
        \]
\end{itemize}
The estimation method used is most-extreme-so-far for both.
